# Run SMC
for(t in 2:n) # time iterations
{
# resample if ESS too small
if(ESS(log.norm.weights, N) < resampling.threshold)
{
# resample and change history of resampled particles
particles <- ResampleAndChangeParticleHistory(log.norm.weights, particles, N, t-1)
log.un.weights <- rep(log(1), N) # reset the weights
}
for(p in 1:N) # particle iterations
{
# proposal allocation
if(p >= 2)
{
# proposal only uses particle up to time t-1
particles[p, t] <- Proposal(sigma, s, particles[p, 1:(t-1)], all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
}
# unnormalised weights
log.un.weights[p] <- LogUnnormalisedWeight(sigma, s, particles[p, 1:t],
log.previous.weight = log.un.weights[p],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t, n, alpha, beta1, beta2)
}
}
# normalised weights
log.norm.weights <- sapply(log.un.weights, function(x){x - logSumExp(log.un.weights)})
particles # 7 7
exp(log.norm.weights)
rm(list = ls())
library(Matrix)
library(igraph)
library(matrixStats)
library(LaplacesDemon)
source("PGSMsFunctions.R")
#****************************************************
# generate SBM
n <- 20 # no. nodes
K <- 10  # no. clusters
num.clusters <- 4
sbm <- sample_sbm(n = 20,
pref.matrix = forceSymmetric(matrix(runif(num.clusters^2),
(c(num.clusters, num.clusters)))),
block.sizes = c(6, 4, 7, 3), directed = FALSE, loops = FALSE); plot(sbm)
sbm <- sample_sbm(n = 20, pref.matrix = 0.99 * diag(1),
block.sizes = 20, directed = FALSE, loops = FALSE); plot(sbm)
rm(list = ls())
library(Matrix)
library(igraph)
library(matrixStats)
library(LaplacesDemon)
source("PGSMsFunctions.R")
#****************************************************
# generate SBM
n <- 20 # no. nodes
K <- 10  # no. clusters
pref.matrix <- diag(K) # Bernoulli rates (K x K matrix)
block.sizes <- rep(n/K, K) # no. nodes in each cluster (K length vector)
sbm <- sample_sbm(n = 20, pref.matrix = 0.99 * diag(1),
block.sizes = 20, directed = FALSE, loops = FALSE); plot(sbm)
start.clusters <- list(1:3, 4:7, 8:10, 11:13, 14:17, 18:20)
alpha <- 1 # Dirichlet process parameter
beta1 <- 1 # Flat uniform priors (McDaid: conjugate priors on the parameter for each cluster)
beta2 <- 1
N <- 20    # no. particles: (Bouchard uses 20)
resampling.threshold <- 0.5
n.iters <- 100000
# perform PGSMs
clusters <- start.clusters
all.clusters <- start.clusters
all.clusters
s <- SelectAnchors(all.clusters)  # select anchors uniform
s
s <- SelectAnchors(all.clusters)  # select anchors uniform
s
closure <- CalculateClosureOfAnchors(s, all.clusters)
c.bar <- closure$c.bar
s.bar <- closure$s.bar
# calculate non.c.bar
non.c.bar.cluster.indicators <- lapply(lapply(all.clusters,
function(x){s %in% x}),
function(y){any(y == TRUE)})
non.c.bar <- all.clusters[which(non.c.bar.cluster.indicators == FALSE)]
# uniform permutation on elements of s.bar - with anchors 1st and 2nd
sigma <- SamplePermutation(s, s.bar)
n <- length(sigma)
# pre-compute cumulative sum of edge counts
cum.sum.counts <- PreComputeCumulativeSumOfEdgeCounts(sigma, non.c.bar, adj)
cum.sum.edge.counts <- cum.sum.counts$cum.sum.edge.counts
cum.sum.max.counts <- cum.sum.counts$cum.sum.max.counts
# define particle matrix & fix 1st particle of each generation to conditional path
particles <- matrix(rep(0, N*n), c(N, n))
particles[1,] <- MapClustersToAllocations(sigma, c.bar) # row 1: particle with conditional path
particles[,1] <- rep(1, N) # column 1: 1st decision is (#1) initialise for all particles
# define weights at t=1
log.un.weights <- rep(log(1), N)
log.norm.weights <- rep(log(1/N), N) # 1st log norm weight is log(1/N) for all particles
# Run SMC
for(t in 2:n) # time iterations
{
# resample if ESS too small
if(ESS(log.norm.weights, N) < resampling.threshold)
{
# resample and change history of resampled particles
particles <- ResampleAndChangeParticleHistory(log.norm.weights, particles, N, t-1)
log.un.weights <- rep(log(1), N) # reset the weights
}
for(p in 1:N) # particle iterations
{
# proposal allocation
if(p >= 2)
{
# proposal only uses particle up to time t-1
particles[p, t] <- Proposal(sigma, s, particles[p, 1:(t-1)], all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
}
# unnormalised weights
log.un.weights[p] <- LogUnnormalisedWeight(sigma, s, particles[p, 1:t],
log.previous.weight = log.un.weights[p],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t, n, alpha, beta1, beta2)
}
}
# normalised weights
log.norm.weights <- sapply(log.un.weights, function(x){x - logSumExp(log.un.weights)})
particles # 7 7
exp(log.norm.weights)
adj <- as_adj(sbm)
closure <- CalculateClosureOfAnchors(s, all.clusters)
c.bar <- closure$c.bar
s.bar <- closure$s.bar
# calculate non.c.bar
non.c.bar.cluster.indicators <- lapply(lapply(all.clusters,
function(x){s %in% x}),
function(y){any(y == TRUE)})
non.c.bar <- all.clusters[which(non.c.bar.cluster.indicators == FALSE)]
# uniform permutation on elements of s.bar - with anchors 1st and 2nd
sigma <- SamplePermutation(s, s.bar)
n <- length(sigma)
# pre-compute cumulative sum of edge counts
cum.sum.counts <- PreComputeCumulativeSumOfEdgeCounts(sigma, non.c.bar, adj)
cum.sum.edge.counts <- cum.sum.counts$cum.sum.edge.counts
cum.sum.max.counts <- cum.sum.counts$cum.sum.max.counts
# define particle matrix & fix 1st particle of each generation to conditional path
particles <- matrix(rep(0, N*n), c(N, n))
particles[1,] <- MapClustersToAllocations(sigma, c.bar) # row 1: particle with conditional path
particles[,1] <- rep(1, N) # column 1: 1st decision is (#1) initialise for all particles
# define weights at t=1
log.un.weights <- rep(log(1), N)
log.norm.weights <- rep(log(1/N), N) # 1st log norm weight is log(1/N) for all particles
# Run SMC
for(t in 2:n) # time iterations
{
# resample if ESS too small
if(ESS(log.norm.weights, N) < resampling.threshold)
{
# resample and change history of resampled particles
particles <- ResampleAndChangeParticleHistory(log.norm.weights, particles, N, t-1)
log.un.weights <- rep(log(1), N) # reset the weights
}
for(p in 1:N) # particle iterations
{
# proposal allocation
if(p >= 2)
{
# proposal only uses particle up to time t-1
particles[p, t] <- Proposal(sigma, s, particles[p, 1:(t-1)], all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
}
# unnormalised weights
log.un.weights[p] <- LogUnnormalisedWeight(sigma, s, particles[p, 1:t],
log.previous.weight = log.un.weights[p],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t, n, alpha, beta1, beta2)
}
}
# normalised weights
log.norm.weights <- sapply(log.un.weights, function(x){x - logSumExp(log.un.weights)})
particles # 7 7
exp(log.norm.weights)
non.c.bar
c.bar
n <- length(sigma)
edge.counts <- rep(0, n)
for(i in 2:n)
{
edge.counts[i] <- sum(adj[sigma[i], sigma[1:(i-1)]])
}
max.counts <- 0:(n-1)
PreComputeEdgeCountsWithinAndBetweenCbar(sigma, adj)
adj[sigma[i], sigma[1:(i-1)]]
PreComputeEdgeCountsBetweenCbarAndNonCbarClusters(sigma, non.c.bar, adj)
c.bar
sigma
non.c.bar
PreComputeEdgeCountsWithinAndBetweenCbar(sigma, adj)
PreComputeEdgeCountsBetweenCbarAndNonCbarClusters(sigma, non.c.bar, adj)
PreComputeCumulativeSumOfEdgeCounts(sigma, non.c.bar, adj)
t
particles
exp(log.norm.weights)
log.un.weights
all.clusters <- start.clusters
all.clusters
s <- SelectAnchors(all.clusters)  # select anchors uniform
s
# calculate c.bar and s.bar
closure <- CalculateClosureOfAnchors(s, all.clusters)
c.bar <- closure$c.bar
s.bar <- closure$s.bar
# calculate non.c.bar
non.c.bar.cluster.indicators <- lapply(lapply(all.clusters,
function(x){s %in% x}),
function(y){any(y == TRUE)})
non.c.bar <- all.clusters[which(non.c.bar.cluster.indicators == FALSE)]
# uniform permutation on elements of s.bar - with anchors 1st and 2nd
sigma <- SamplePermutation(s, s.bar)
n <- length(sigma)
# pre-compute cumulative sum of edge counts
cum.sum.counts <- PreComputeCumulativeSumOfEdgeCounts(sigma, non.c.bar, adj)
cum.sum.edge.counts <- cum.sum.counts$cum.sum.edge.counts
cum.sum.max.counts <- cum.sum.counts$cum.sum.max.counts
# define particle matrix & fix 1st particle of each generation to conditional path
particles <- matrix(rep(0, N*n), c(N, n))
particles[1,] <- MapClustersToAllocations(sigma, c.bar) # row 1: particle with conditional path
particles[,1] <- rep(1, N) # column 1: 1st decision is (#1) initialise for all particles
# define weights at t=1
log.un.weights <- rep(log(1), N)
log.norm.weights <- rep(log(1/N), N) # 1st l
t=2
print(paste("resampled at iteration:", t, sep = " "))
t
for(p in 1:N) # particle iterations
{
# proposal allocation
if(p >= 2)
{
# proposal only uses particle up to time t-1
particles[p, t] <- Proposal(sigma, s, particles[p, 1:(t-1)], all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
}
# unnormalised weights
log.un.weights[p] <- LogUnnormalisedWeight(sigma, s, particles[p, 1:t],
log.previous.weight = log.un.weights[p],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t, n, alpha, beta1, beta2)
}
particles
log.un.weights
t=3
ESS(log.norm.weights, N)
for(p in 1:N) # particle iterations
{
# proposal allocation
if(p >= 2)
{
# proposal only uses particle up to time t-1
particles[p, t] <- Proposal(sigma, s, particles[p, 1:(t-1)], all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
}
# unnormalised weights
log.un.weights[p] <- LogUnnormalisedWeight(sigma, s, particles[p, 1:t],
log.previous.weight = log.un.weights[p],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t, n, alpha, beta1, beta2)
}
particles
log.un.weights
t
particles[p, 1:t]
particles[19, 1:t]
particle = particles[19, 1:t]
particle
particle1 = particle
particle2 = c(1, 2, 2)
p1 = particle1
p2 = particle2
p1
p2
particles
t=4
for(p in 1:N) # particle iterations
{
# proposal allocation
if(p >= 2)
{
# proposal only uses particle up to time t-1
particles[p, t] <- Proposal(sigma, s, particles[p, 1:(t-1)], all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
}
# unnormalised weights
# log.un.weights[p] <- LogUnnormalisedWeight(sigma, s, particles[p, 1:t],
#                                            log.previous.weight = log.un.weights[p],
#                                            all.clusters, cum.sum.edge.counts,
#                                            cum.sum.max.counts, adj, tau1, tau2,
#                                            t, n, alpha, beta1, beta2)
}
particles
log.un.weights
lpw1 = lpw2 = log.un.weights[1]
lpw1
p1
p1 = c(1,4,3,4)
p2 = c(1,2,2,2)
lpw1
lpw2
p1
p2
particle = p1
PossibleAllocations(sigma, s, particle, all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
LogImprovedIntermediateTarget(sigma, s, particle[1:(t-1)],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t-1, n, alpha, beta1, beta2)
PossibleAllocations(sigma, s, particle, all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
particle = p2
p2
PossibleAllocations(sigma, s, particle, all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
LogImprovedIntermediateTarget(sigma, s, particle[1:(t-1)],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t-1, n, alpha, beta1, beta2)
LogImprovedIntermediateTarget(sigma, s, particle,
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t-1, n, alpha, beta1, beta2)
particle
particle = p1
p1
LogImprovedIntermediateTarget(sigma, s, particle[1:(t-1)],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t-1, n, alpha, beta1, beta2)
particle
particle = p2
p2
t
possible.allocations <- PossibleAllocations(sigma, s, particle, all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
# gamma.hat at t-1
log.gamma.hat.previous <- LogImprovedIntermediateTarget(sigma, s, particle[1:(t-1)],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t-1, n, alpha, beta1, beta2)
log.weight.update <- possible.allocations$log.stay.gamma.hat - log.gamma.hat.previous
log.weight.update
log.weight.update2 = log.weight.update
log.weight.update2
particle
particle = p1
p1
possible.allocations <- PossibleAllocations(sigma, s, particle, all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
# gamma.hat at t-1
log.gamma.hat.previous <- LogImprovedIntermediateTarget(sigma, s, particle[1:(t-1)],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t-1, n, alpha, beta1, beta2)
log.weight.update <- logSumExp(c(possible.allocations$log.stay.gamma.hat,
possible.allocations$log.move.gamma.hat)) -
log.gamma.hat.previous
log.weight.update
log.weight.update <- logSumExp(c(possible.allocations$log.stay.gamma.hat,
possible.allocations$log.move.gamma.hat)) - log.gamma.hat.previous
log.weight.update
particle
p1
possible.allocations <- PossibleAllocations(sigma, s, particle, all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
possible.allocations1 <- PossibleAllocations(sigma, s, particle, all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
log.gamma.hat.previous1 <- LogImprovedIntermediateTarget(sigma, s, particle[1:(t-1)],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t-1, n, alpha, beta1, beta2)
particle = p2
p2
possible.allocations2 <- PossibleAllocations(sigma, s, particle, all.clusters,
cum.sum.edge.counts, cum.sum.max.counts,
adj, tau1, tau2, t, n, alpha, beta1, beta2)
log.gamma.hat.previous2 <- LogImprovedIntermediateTarget(sigma, s, particle[1:(t-1)],
all.clusters, cum.sum.edge.counts,
cum.sum.max.counts, adj, tau1, tau2,
t-1, n, alpha, beta1, beta2)
possible.allocations1
possible.allocations2
log.gamma.hat.previous1
log.gamma.hat.previous2
log.weight.update2 <- possible.allocations2$log.stay.gamma.hat - log.gamma.hat.previous2
log.weight.update2
log.weight.update1 <- logSumExp(c(possible.allocations1$log.stay.gamma.hat,
possible.allocations1$log.move.gamma.hat)) - log.gamma.hat.previous1
log.weight.update1
logSumExp(c(possible.allocations1$log.stay.gamma.hat,
possible.allocations1$log.move.gamma.hat))
possible.allocations2$log.stay.gamma.hat
p1
p2
possible.allocations1
possible.allocations2
exp9-0.28
exp(-0.28)
log.gamma.hat.previous1
log.gamma.hat.previous2
log.weight.update1
log.weight.update2
log.weight.update1 <- possible.allocations1$log.stay.gamma.hat - log.gamma.hat.previous1
p1
p2
log.weight.update2 <- possible.allocations2$log.stay.gamma.hat - log.gamma.hat.previous2
log.weight.update1 <- logSumExp(c(possible.allocations1$log.stay.gamma.hat,
possible.allocations1$log.move.gamma.hat)) - log.gamma.hat.previous1
log.weight.update1 <- logSumExp(c(possible.allocations1$log.stay.gamma.hat,
possible.allocations1$log.move.gamma.hat)) - log.gamma.hat.previous1
log.weight.update1
log.weight.update2
possible.allocations2$log.stay.gamma.hat - log.gamma.hat.previous2
possible.allocations2$log.stay.gamma.hat
- log.gamma.hat.previous2
log.gamma.hat.previous2
logSumExp(c(possible.allocations1$log.stay.gamma.hat,
possible.allocations1$log.move.gamma.hat)) - log.gamma.hat.previous1
logSumExp(c(possible.allocations1$log.stay.gamma.hat,
possible.allocations1$log.move.gamma.hat))
log.gamma.hat.previous1
logSumExp(c(possible.allocations1$log.stay.gamma.hat,
possible.allocations1$log.move.gamma.hat)) - log.gamma.hat.previous1
-0.9796024 - -1.095437
log.weight.update1
log.weight.update2
log.weight.update1 - log.weight.update2
possible.allocations1
possible.allocations2
gfjgfj
log.gamma.hat.previous1
gfjgfj
log.gamma.hat.previous2
log.gamma.hat.previous2 + log.gamma.hat.previous1
particles
#****************************************************
rm(list = ls())
library(Matrix)
library(igraph)
library(matrixStats)
library(LaplacesDemon)
source("PGSMsFunctions.R")
#****************************************************
# generate SBM
n <- 20 # no. nodes
K <- 10  # no. clusters
pref.matrix <- diag(K) # Bernoulli rates (K x K matrix)
block.sizes <- rep(n/K, K) # no. nodes in each cluster (K length vector)
sbm <- sample_sbm(n, pref.matrix, block.sizes, directed = FALSE, loops = FALSE); plot(sbm)
start.clusters <- list(1:n) # list of clusters
adj <- as_adj(sbm)
# PGSMs tuning parameters
alpha <- 1 # Dirichlet process parameter
beta1 <- 1 # Flat uniform priors (McDaid: conjugate priors on the parameter for each cluster)
beta2 <- 1
N <- 20    # no. particles: (Bouchard uses 20)
resampling.threshold <- 0.5
n.iters <- 100000
# perform PGSMs
clusters <- start.clusters
all.clusters <- start.clusters
all.clusters
s <- SelectAnchors(all.clusters)  # select anchors uniform
s
# calculate c.bar and s.bar
closure <- CalculateClosureOfAnchors(s, all.clusters)
c.bar <- closure$c.bar
s.bar <- closure$s.bar
# calculate non.c.bar
non.c.bar.cluster.indicators <- lapply(lapply(all.clusters,
function(x){s %in% x}),
function(y){any(y == TRUE)})
non.c.bar <- all.clusters[which(non.c.bar.cluster.indicators == FALSE)]
# uniform permutation on elements of s.bar - with anchors 1st and 2nd
sigma <- SamplePermutation(s, s.bar)
n <- length(sigma)
# pre-compute cumulative sum of edge counts
cum.sum.counts <- PreComputeCumulativeSumOfEdgeCounts(sigma, non.c.bar, adj)
cum.sum.edge.counts <- cum.sum.counts$cum.sum.edge.counts
cum.sum.max.counts <- cum.sum.counts$cum.sum.max.counts
cum.sum.max.counts
cum.sum.edge.counts
cum.sum.edge.counts
cum.sum.counts$cum.sum.edge.counts
cum.sum.counts
all.clusters
